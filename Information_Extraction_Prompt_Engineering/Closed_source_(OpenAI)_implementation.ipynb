{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeBixcvQhjvC"
      },
      "source": [
        "# Prompt Engineering with closed-source (OpenAI) Large Language Models\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup and Installation\n",
        "2. Set OpenAI API key and Response Function]\n",
        "3. Normalization Functions\n",
        "4. Evaluation Functions\n",
        "5. Structured Prompts\n",
        "6. Zero-shot Prompting\n",
        "7. Few-shot Prompting\n",
        "8. Chain-of-Thought Prompting\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LktXz1fEtSTo"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A6ACq8mhmP0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJSzWJowtYhV"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQP9oCuFt6CK"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "import re\n",
        "from difflib import SequenceMatcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO7wWFoCt_fY"
      },
      "outputs": [],
      "source": [
        "# Set Working Directory and read csv file\n",
        "os.chdir(r'/content/drive/MyDrive')\n",
        "df = pd.read_csv(r'Extraction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC6inHXGuyN2"
      },
      "source": [
        "## 2. Set OpenAI API key and Response Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzX_WA0nu4hU"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_AI_KEY = userdata.get('OPEN_AI_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_AI_KEY\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBAXBf0e1T_l"
      },
      "outputs": [],
      "source": [
        "# Define your response function using gpt-4o\n",
        "def generate_response(prompt, model=\"gpt-4o\", temperature=0, max_tokens=300, top_p=1.0, **kwargs):\n",
        "    \"\"\"Generate a response using OpenAI's API.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        **kwargs\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM7bePkzO12A"
      },
      "source": [
        "## 3. Normalization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKtblDs5HtiS"
      },
      "outputs": [],
      "source": [
        "# Function to correct JSON format output and print outputs from model and ground truth\n",
        "def format_and_print_json(gpt_output, ground_truth, title_1=\"üîπ GPT Output\", title_2=\"üî∏ Ground Truth\"):\n",
        "    \"\"\"\n",
        "    Normalize and pretty-print GPT and ground truth JSON for side-by-side comparison.\n",
        "\n",
        "    Steps:\n",
        "    - Converts inputs to Python dictionaries if they're JSON strings.\n",
        "    - Sorts list values (e.g., 'party') alphabetically for consistent display.\n",
        "    - Pretty-prints both outputs with proper indentation and optional Markdown formatting.\n",
        "\n",
        "    Args:\n",
        "        gpt_output (str or dict): GPT model's output\n",
        "        ground_truth (str or dict): Ground truth metadata\n",
        "        title_1 (str): Optional title for GPT output\n",
        "        title_2 (str): Optional title for ground truth\n",
        "    \"\"\"\n",
        "    # Convert strings to dicts if needed\n",
        "    if isinstance(gpt_output, str):\n",
        "        try:\n",
        "            gpt_output = json.loads(gpt_output)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"‚ö†Ô∏è GPT Output is not valid JSON\")\n",
        "            return\n",
        "    if isinstance(ground_truth, str):\n",
        "        ground_truth = json.loads(ground_truth)\n",
        "\n",
        "    # Sort lists in each key\n",
        "    def sort_lists(obj):\n",
        "        for key, val in obj.items():\n",
        "            if isinstance(val, list):\n",
        "                obj[key] = sorted(val)\n",
        "        return obj\n",
        "\n",
        "    gpt_output = sort_lists(gpt_output)\n",
        "    ground_truth = sort_lists(ground_truth)\n",
        "\n",
        "    # Pretty print JSON\n",
        "    print(f\"{title_1}:\\n```json\\n{json.dumps(gpt_output, indent=4)}\\n```\")\n",
        "    print(f\"\\n{title_2}:\\n```json\\n{json.dumps(ground_truth, indent=4)}\\n```\")\n",
        "\n",
        "# Extract the first valid JSON object from GPT response\n",
        "def extract_json_from_text(text):\n",
        "    try:\n",
        "        match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
        "        if match:\n",
        "            return match.group()\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è JSON extraction error:\", str(e))\n",
        "    raise ValueError(\"No valid JSON found in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97w6stIiUorm"
      },
      "source": [
        "## 4. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-PzvzUOVQC"
      },
      "outputs": [],
      "source": [
        "# Field-by-field value Accuracy\n",
        "def compare_parties(pred, truth):\n",
        "    \"\"\"\n",
        "    Compare two lists of 'party' entities using Jaccard similarity (set overlap).\n",
        "\n",
        "    Parameters:\n",
        "        pred (list): List of parties predicted by the model.\n",
        "        truth (list): List of ground truth parties.\n",
        "\n",
        "    Returns:\n",
        "        float: A score between 0 and 1 representing similarity.\n",
        "            - 1.0 if both sets match exactly\n",
        "            - 0.0 if no overlap or one is empty\n",
        "            - Partial score if there's partial overlap (e.g., 1 common out of 3 total = 0.33)\n",
        "    \"\"\"\n",
        "    pred_set = set(pred)\n",
        "    truth_set = set(truth)\n",
        "    if not pred_set and not truth_set:\n",
        "        return 1.0\n",
        "    if not pred_set or not truth_set:\n",
        "        return 0.0\n",
        "    intersection = pred_set & truth_set\n",
        "    union = pred_set | truth_set\n",
        "    return round(len(intersection) / len(union), 2)\n",
        "\n",
        "\n",
        "def evaluate_metadata_fields(gpt_output, ground_truth):\n",
        "    \"\"\"\n",
        "    Evaluate metadata extraction quality by comparing the GPT output with the ground truth on a per-field basis.\n",
        "\n",
        "    This function checks the accuracy of each metadata item:\n",
        "    - effective_date, jurisdiction, party, term\n",
        "    - Handles optional fields (no penalty if both are missing)\n",
        "    - For 'party', it allows partial credit using set overlap (Jaccard similarity)\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with per-field accuracy scores:\n",
        "        - 1.0 for exact matches\n",
        "        - 0.0 for mismatches\n",
        "        - partial value for list overlaps (e.g., 'party')\n",
        "        - None if both fields are missing\n",
        "    \"\"\"\n",
        "    if isinstance(gpt_output, str):\n",
        "        gpt_output = json.loads(gpt_output)\n",
        "    if isinstance(ground_truth, str):\n",
        "        ground_truth = json.loads(ground_truth)\n",
        "\n",
        "    fields = ['effective_date', 'jurisdiction', 'party', 'term']\n",
        "    field_scores = {}\n",
        "\n",
        "    for field in fields:\n",
        "        gpt_val = gpt_output.get(field)\n",
        "        gt_val = ground_truth.get(field)\n",
        "\n",
        "        if gpt_val is None and gt_val is None:\n",
        "            field_scores[field] = None  # No penalty if missing in both\n",
        "        elif gpt_val is None or gt_val is None:\n",
        "            field_scores[field] = 0.0\n",
        "        else:\n",
        "            if field != \"party\":\n",
        "                field_scores[field] = 1.0 if gpt_val == gt_val else 0.0\n",
        "            else:\n",
        "                field_scores[field] = compare_parties(gpt_val, gt_val)\n",
        "\n",
        "    return field_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3CQrwB4TCc9"
      },
      "outputs": [],
      "source": [
        "# Evaluate key_match and party_count\n",
        "def evaluate_key_match_and_party_count(gpt_output, true_keys_str, true_party_count):\n",
        "    \"\"\"\n",
        "    Evaluate the structural correctness of GPT output based on:\n",
        "    - Whether all expected metadata fields (keys) are present\n",
        "    - Whether the number of 'party' entities matches the ground truth\n",
        "\n",
        "    Parameters:\n",
        "        gpt_output: The extracted metadata from GPT\n",
        "        true_keys_str: A comma-separated string of expected keys (from df['keys'])\n",
        "        true_party_count: The number of expected parties (from df['party_count'])\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with:\n",
        "        - key_match_score: Fraction of required keys that GPT included\n",
        "        - party_count_score: 1 if correct number of parties, 0 otherwise\n",
        "        - supporting details like predicted/expected keys and party counts\n",
        "    \"\"\"\n",
        "    if isinstance(gpt_output, str):\n",
        "        gpt_output = json.loads(gpt_output)\n",
        "\n",
        "    expected_keys = set(true_keys_str.split(','))\n",
        "    gpt_keys = set(gpt_output.keys())\n",
        "\n",
        "    matched_keys = expected_keys & gpt_keys\n",
        "    key_match_score = round(len(matched_keys) / len(expected_keys), 2)\n",
        "\n",
        "    predicted_party_count = len(gpt_output.get('party', [])) if 'party' in gpt_output else 0\n",
        "    party_count_score = 1.0 if predicted_party_count == true_party_count else 0.0\n",
        "\n",
        "    return {\n",
        "        \"key_match_score\": key_match_score,\n",
        "        \"party_count_score\": party_count_score,\n",
        "        \"expected_keys\": expected_keys,\n",
        "        \"returned_keys\": gpt_keys,\n",
        "        \"expected_party_count\": true_party_count,\n",
        "        \"predicted_party_count\": predicted_party_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-F-hq_Obo_7"
      },
      "outputs": [],
      "source": [
        "def summarize_evaluation_across_samples(all_field_scores, all_structural_scores):\n",
        "    \"\"\"\n",
        "    Summarizes evaluation metrics across multiple samples.\n",
        "    Handles missing fields and avoids ZeroDivisionError.\n",
        "    \"\"\"\n",
        "    total_field_scores = {'effective_date': [], 'jurisdiction': [], 'party': [], 'term': []}\n",
        "    total_structural_scores = {'key_match': [], 'party_count': []}\n",
        "\n",
        "    print(\"\\n\\n================== üßæ Evaluation Summary Across Samples ==================\\n\")\n",
        "\n",
        "    for i, (field_scores, structural) in enumerate(zip(all_field_scores, all_structural_scores)):\n",
        "        print(f\"\\nüìÑ Sample {i+1}\")\n",
        "\n",
        "        # --- Field-Level Scores ---\n",
        "        field_sum = 0\n",
        "        field_count = 0\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"- {field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"- {field}: {score}\")\n",
        "                total_field_scores[field].append(score)\n",
        "                field_sum += score\n",
        "                field_count += 1\n",
        "\n",
        "        avg_field_score = round(field_sum / field_count, 3) if field_count > 0 else \"N/A\"\n",
        "\n",
        "        # --- Structural Scores ---\n",
        "        key_score = structural.get('key_match_score')\n",
        "        party_score = structural.get('party_count_score')\n",
        "\n",
        "        if key_score is not None:\n",
        "            total_structural_scores['key_match'].append(key_score)\n",
        "        if party_score is not None:\n",
        "            total_structural_scores['party_count'].append(party_score)\n",
        "\n",
        "        print(f\"- Key Match Score: {key_score}\")\n",
        "        print(f\"- Party Count Score: {party_score}\")\n",
        "\n",
        "        combined_structural = [\n",
        "            s for s in [key_score, party_score] if s is not None\n",
        "        ]\n",
        "        avg_structural = round(sum(combined_structural) / len(combined_structural), 3) if combined_structural else \"N/A\"\n",
        "\n",
        "        print(f\"üîπ Avg Field Score: {avg_field_score}\")\n",
        "        print(f\"üî∏ Combined Structural Score: {avg_structural}\")\n",
        "\n",
        "    # --- Aggregate Averages ---\n",
        "    print(\"\\n================== üìä AVERAGES ==================\\n\")\n",
        "\n",
        "    print(\"üîπ Field-Level Averages:\")\n",
        "    for field, scores in total_field_scores.items():\n",
        "        if scores:\n",
        "            avg = round(sum(scores) / len(scores), 3)\n",
        "            print(f\"- {field}: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {field}: (No available comparisons)\")\n",
        "\n",
        "    print(\"\\nüî∏ Structural Averages:\")\n",
        "    for k in total_structural_scores:\n",
        "        scores = total_structural_scores[k]\n",
        "        if scores:\n",
        "            avg = round(sum(scores) / len(scores), 3)\n",
        "            label = \"Key Match\" if k == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: {avg}\")\n",
        "        else:\n",
        "            label = \"Key Match\" if k == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: (No available comparisons)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb8BbZhw3_6Z"
      },
      "outputs": [],
      "source": [
        "def print_average_scores_only(all_field_scores, all_structural_scores):\n",
        "    \"\"\"\n",
        "    Print only the average field-level and structural scores across all samples.\n",
        "    Use this when you want a compact summary view.\n",
        "    \"\"\"\n",
        "    print(\"\\n================== üìä AVERAGE METRICS ==================\\n\")\n",
        "\n",
        "    # --- Field-Level Averages ---\n",
        "    field_totals = {}\n",
        "    field_counts = {}\n",
        "\n",
        "    for sample in all_field_scores:\n",
        "        for field, score in sample.items():\n",
        "            if score is not None:\n",
        "                field_totals[field] = field_totals.get(field, 0) + score\n",
        "                field_counts[field] = field_counts.get(field, 0) + 1\n",
        "\n",
        "    print(\"üîπ Field-Level Averages:\")\n",
        "    for field in ['effective_date', 'jurisdiction', 'party', 'term']:\n",
        "        if field_counts.get(field, 0) > 0:\n",
        "            avg = round(field_totals[field] / field_counts[field], 3)\n",
        "            print(f\"- {field}: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {field}: (No available comparisons)\")\n",
        "\n",
        "    # --- Structural Averages ---\n",
        "    total_structural_scores = {\n",
        "        \"key_match\": [],\n",
        "        \"party_count\": []\n",
        "    }\n",
        "\n",
        "    for result in all_structural_scores:\n",
        "        total_structural_scores[\"key_match\"].append(result[\"key_match_score\"])\n",
        "        total_structural_scores[\"party_count\"].append(result[\"party_count_score\"])\n",
        "\n",
        "    print(\"\\nüî∏ Structural Averages:\")\n",
        "    for metric_key, values in total_structural_scores.items():\n",
        "        if values:\n",
        "            avg = round(sum(values) / len(values), 3)\n",
        "            label = \"Key Match\" if metric_key == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {metric_key} Score: (No data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h03CR4S58h1W"
      },
      "source": [
        "## 5. Structured Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeOx0yQv0SPk"
      },
      "outputs": [],
      "source": [
        "# Define persona, instructions, and formatting rules\n",
        "persona = \"\"\"\n",
        "You are an expert in identifying and extracting metadata from NDA (Non-Disclosure Agreement) documents.\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"\n",
        "Extract the following metadata:\n",
        "- effective_date: When the agreement becomes legally binding\n",
        "- jurisdiction: The governing legal territory\n",
        "- party: Entities bound by the agreement (May have multiple values)\n",
        "- term: Duration of the agreement\n",
        "Return only the metadata fields that are explicitly present in the text. If a field like effective_date or term is missing, exclude it from the output JSON and do not guess or explain.\n",
        "\"\"\"\n",
        "\n",
        "data_format = \"\"\"\n",
        "Output must be a JSON object following these Rules:\n",
        "- Replace all spaces and colons (:) in attribute values with underscores (_)\n",
        "- Dates must be in 'YYYY-MM-DD' format\n",
        "- Jurisdiction: Only the state or country name is included, without prefixes like \"State of\"\n",
        "- Party names must follow Title_Case with underscores replacing spaces and corporate suffixes standardized (e.g., \"Inc.\", \"Corp.\", \"LLC\", \"Ltd.\")\n",
        "- Duration (term) must be normalized to 'number_units' format (e.g., 2_years, 12_months)\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQhGaY7w8VeG"
      },
      "source": [
        "## 6. Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV_FrKnJVHqy",
        "outputId": "57a86d9f-d4f2-4863-969f-a70d87abe90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_&_Struggles_Inc\",\n",
            "        \"Second_Party\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Limited\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"GTCR_LLC\",\n",
            "        \"Vocus_Inc\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"Gtcr_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industries_Inc.\"\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: (Not present in either) ‚úÖ\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.75\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.777\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.667\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "üìÑ Sample 4\n",
            "- effective_date: (Not present in either) ‚úÖ\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: 0.0\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.443\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.165\n",
            "- term: 0.5\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.875\n",
            "- Party Count Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot evaluation: no examples included in the prompt\n",
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    # Zero-shot prompt\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract clean JSON from GPT response\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        # Display GPT vs Ground Truth\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        # Evaluation Method 1: Field-Level Accuracy\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        # Evaluation Method 2: Structural Correctness\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è GPT Output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary for zero-shot\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUtu7oNo9OiM"
      },
      "source": [
        "## 7. Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjpJtZPbGG1",
        "outputId": "68b314a7-ec0d-4d9c-fa2c-bd4249c9a019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_&_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Limited\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"GTCR_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"Gtcr_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industries_Inc.\"\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: (Not present in either) ‚úÖ\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.833\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.777\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.777\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "üìÑ Sample 4\n",
            "- effective_date: (Not present in either) ‚úÖ\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: 0.0\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.443\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: 0.5\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.875\n",
            "- Party Count Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Few-shot examples\n",
        "examples = \"\"\n",
        "for i in range(3):  # Adjust number of few-shot examples here\n",
        "    text_sample = df['text'][i]\n",
        "    metadata_sample = df['extracted'][i]\n",
        "    examples += f\"\\n\\nExample {i+1}\\ntext = {text_sample}\\noutput = {metadata_sample}\"\n",
        "\n",
        "# Main prompt test on selected samples\n",
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    prompt1 = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Here are some examples of NDA metadata extraction:{examples}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt1)\n",
        "\n",
        "    try:\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è GPT Output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AClDDTrnfMPr"
      },
      "source": [
        "## 8. Chain-of-Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dECN2WZeJBI",
        "outputId": "7bb6bd37-3884-4cb8-be65-0acb9f4fca2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_&_Struggles_Inc\",\n",
            "        \"Second_Party\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Limited\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"GTCR_LLC\",\n",
            "        \"Vocus_Inc\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"Gtcr_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n",
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2005-11-01\",\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industries_Inc.\"\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 0.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Returned Keys: {'jurisdiction', 'effective_date', 'term', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.75\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.777\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.667\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "üìÑ Sample 4\n",
            "- effective_date: 0.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: 0.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.333\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.75\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.165\n",
            "- term: 0.5\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.938\n",
            "- Party Count Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot evaluation: no examples included in the prompt\n",
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    # Zero-shot prompt\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Let‚Äôs extract the required metadata by thinking step-by-step.\n",
        "\n",
        "1. First, read the NDA and look for the effective_date. This is typically found where the agreement says something like \"effective as of\", \"made on\", or \"entered into on\".\n",
        "2. Next, identify the jurisdiction. This is usually mentioned in a clause like \"governed by the laws of...\" or \"jurisdiction of\".\n",
        "3. Then, extract all the parties. These are the legal entities entering into the agreement. They often appear at the beginning or near the signature lines.\n",
        "4. Finally, determine the term. Look for duration-related phrases like \"for two years\", \"valid until\", or \"will terminate after...\".\n",
        "\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract clean JSON from GPT response\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        # Display GPT vs Ground Truth\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        # Evaluation Method 1: Field-Level Accuracy\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        # Evaluation Method 2: Structural Correctness\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è GPT Output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary for zero-shot\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTLNNSkA4G18"
      },
      "source": [
        "## 9. Evaluation Using Entire Dataset (Few-shot Prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0djBqPHZAW8Y",
        "outputId": "d72cc614-d9fc-4d06-d0d5-5766ba932a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== üìä AVERAGE METRICS ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.936\n",
            "- jurisdiction: 0.99\n",
            "- party: 0.497\n",
            "- term: 0.387\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.858\n",
            "- Party Count Score: 0.875\n"
          ]
        }
      ],
      "source": [
        "# Prepare few-shot examples\n",
        "examples = \"\"\n",
        "for i in range(3):  # Adjust number of few-shot examples here\n",
        "    text_sample = df['text'][i]\n",
        "    metadata_sample = df['extracted'][i]\n",
        "    examples += f\"\\n\\nExample {i+1}\\ntext = {text_sample}\\noutput = {metadata_sample}\"\n",
        "\n",
        "# Run across all rows in the DataFrame\n",
        "example_indices = df.index.tolist()\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "results = []\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    prompt1 = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Here are some examples of NDA metadata extraction:{examples}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    model_output_raw = generate_response(prompt1)\n",
        "\n",
        "    try:\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        results.append({\n",
        "            \"index\": x,\n",
        "            \"model_output\": extracted_json,\n",
        "            \"ground_truth\": df['extracted'][x],\n",
        "            \"model_keys\": key_eval['returned_keys'],\n",
        "            \"true_keys\": key_eval['expected_keys'],\n",
        "            \"model_party_count\": key_eval['predicted_party_count'],\n",
        "            \"true_party_count\": key_eval['expected_party_count']\n",
        "        })\n",
        "\n",
        "    except ValueError:\n",
        "        results.append({\n",
        "            \"index\": x,\n",
        "            \"model_output\": model_output_raw,\n",
        "            \"ground_truth\": df['extracted'][x],\n",
        "            \"model_keys\": \"INVALID_JSON\",\n",
        "            \"true_keys\": df['keys'][x],\n",
        "            \"model_party_count\": \"INVALID_JSON\",\n",
        "            \"true_party_count\": df['party_count'][x]\n",
        "        })\n",
        "\n",
        "# Final Summary\n",
        "print_average_scores_only(all_field_scores, all_structural_scores)\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"openai.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
