{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TKK2afX3FWf"
      },
      "source": [
        "# Prompt Engineering with Open Source Large Language Models\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "1. Setup and Installation\n",
        "2. Mistral-7B-Instruct Response Function\n",
        "3. Normalization Functions\n",
        "4. Evaluation Functions\n",
        "5. Structured Prompts\n",
        "6. Zero-shot Prompting\n",
        "7. Few-shot Prompting\n",
        "8. Chain-of-Thought Prompting\n",
        "9. Evaluation Using Entire Dataset (Few-shot Prompting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2LVTJvW45oN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.49.0 datasets accelerate==1.4.0 vllm==0.8.2 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYpbMfSN9NQK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from pprint import pprint\n",
        "import re\n",
        "from difflib import SequenceMatcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV3OoDMT6qPW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"üîê No HF token found ‚Äì skipping login (only needed for gated/private models)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi7-13ES9Uka"
      },
      "outputs": [],
      "source": [
        "# Set Working Directory and read csv file\n",
        "os.chdir(r'/content/drive/MyDrive)\n",
        "df = pd.read_csv(r'Extraction.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "d94feecce5e94d19856fc8a768c9d39f",
            "d19635d104964246a6a12171ebd5c908",
            "36706232689d43b087b2e8f6123cf189",
            "6d1dc03053d84f6a9534c91a4f761878",
            "b8c89772d0ec48bc96576e40fe15668f",
            "f00c23a9cbe047c68bc0c434c8c0594e",
            "32064c5476a447ba9038ea4d4190f339",
            "351b91f723e44be3bc83c6bacd8bc220",
            "c5b85866acd441efb7b3415a1318c165",
            "8ece49005f4643c684953f1e90b40d40",
            "60e9d2caf01e4695bb857487bfb6f099"
          ]
        },
        "id": "5qAnjIi27orG",
        "outputId": "031add61-70c0-40a1-e88e-b0beecc6b2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-04 01:53:32 [__init__.py:239] Automatically detected platform cuda.\n",
            "WARNING 04-04 01:53:34 [config.py:2614] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-04 01:53:46 [config.py:585] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 04-04 01:53:46 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:25: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode \"mistral\"` to ensure correct encoding and decoding.\n",
            "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-04 01:53:48 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='mistralai/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=10500, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=mistralai/Mistral-7B-Instruct-v0.2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 04-04 01:53:48 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a5ae21c9210>\n",
            "INFO 04-04 01:53:49 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 04-04 01:53:49 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
            "INFO 04-04 01:53:49 [gpu_model_runner.py:1174] Starting to load model mistralai/Mistral-7B-Instruct-v0.2...\n",
            "WARNING 04-04 01:53:49 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 04-04 01:53:49 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d94feecce5e94d19856fc8a768c9d39f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-04 01:53:57 [loader.py:447] Loading weights took 7.60 seconds\n",
            "INFO 04-04 01:53:58 [gpu_model_runner.py:1186] Model loading took 13.4967 GB and 8.186906 seconds\n",
            "INFO 04-04 01:54:09 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/e49581de1a/rank_0_0 for vLLM's torch.compile\n",
            "INFO 04-04 01:54:09 [backends.py:425] Dynamo bytecode transform time: 11.69 s\n",
            "INFO 04-04 01:54:10 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
            "INFO 04-04 01:54:18 [monitor.py:33] torch.compile takes 11.69 s in total\n",
            "INFO 04-04 01:54:19 [kv_cache_utils.py:566] GPU KV cache size: 101,936 tokens\n",
            "INFO 04-04 01:54:19 [kv_cache_utils.py:569] Maximum concurrency for 10,500 tokens per request: 9.71x\n",
            "INFO 04-04 01:54:53 [gpu_model_runner.py:1534] Graph capturing finished in 34 secs, took 0.51 GiB\n",
            "INFO 04-04 01:54:53 [core.py:151] init engine (profile, create kv cache, warmup model) took 55.45 seconds\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load Mixtral with longer context window (32k tokens)\n",
        "llm = LLM(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    tensor_parallel_size=1,\n",
        "    dtype=\"float16\",\n",
        "    gpu_memory_utilization=0.7,\n",
        "    max_model_len=10500,\n",
        "    trust_remote_code=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCPtd9ImnbKL"
      },
      "outputs": [],
      "source": [
        "# Wrapper for inference using Mistral model\n",
        "def safe_str(x):\n",
        "    \"\"\"\n",
        "    Safely converts any input (LLM output) to a clean string.\n",
        "    Handles:\n",
        "    - dict/list ‚Üí JSON string\n",
        "    - None ‚Üí empty string\n",
        "    - str ‚Üí stripped string\n",
        "    - others ‚Üí stringified and stripped\n",
        "    \"\"\"\n",
        "    if isinstance(x, str):\n",
        "        return x.strip()\n",
        "    elif isinstance(x, (dict, list)):\n",
        "        return json.dumps(x)\n",
        "    elif x is None:\n",
        "        return \"\"\n",
        "    else:\n",
        "        return str(x).strip()\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"\n",
        "    Runs prompt through the vLLM model and returns a safe string.\n",
        "    \"\"\"\n",
        "    outputs = llm.generate(prompt, sampling_params)\n",
        "    return safe_str(outputs[0].outputs[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_5onr9_LQ8r"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DpsHq3NHevl"
      },
      "source": [
        "## 3. Normalization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km_0ffcJHgko"
      },
      "outputs": [],
      "source": [
        "# Function to correct JSON format output and print outputs from model and ground truth\n",
        "def format_and_print_json(gpt_output, ground_truth, title_1=\"üîπ Mistralai Output\", title_2=\"üî∏ Ground Truth\"):\n",
        "    \"\"\"\n",
        "    Normalize and pretty-print GPT and ground truth JSON for side-by-side comparison.\n",
        "\n",
        "    Steps:\n",
        "    - Converts inputs to Python dictionaries if they're JSON strings.\n",
        "    - Sorts list values (e.g., 'party') alphabetically for consistent display.\n",
        "    - Pretty-prints both outputs with proper indentation and optional Markdown formatting.\n",
        "\n",
        "    Args:\n",
        "        gpt_output (str or dict): GPT model's output\n",
        "        ground_truth (str or dict): Ground truth metadata\n",
        "        title_1 (str): Optional title for Mistralai output\n",
        "        title_2 (str): Optional title for ground truth\n",
        "    \"\"\"\n",
        "    # Convert strings to dicts if needed\n",
        "    if isinstance(gpt_output, str):\n",
        "        try:\n",
        "            gpt_output = json.loads(gpt_output)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"‚ö†Ô∏è Mistralai Output is not valid JSON\")\n",
        "            return\n",
        "    if isinstance(ground_truth, str):\n",
        "        ground_truth = json.loads(ground_truth)\n",
        "\n",
        "    # Helper function to sort lists\n",
        "    def sort_lists(obj):\n",
        "        for key, val in obj.items():\n",
        "            if isinstance(val, list):\n",
        "                if all(isinstance(item, dict) for item in val):\n",
        "                    # Sort list of dicts by JSON string representation\n",
        "                    obj[key] = sorted(val, key=lambda d: json.dumps(d, sort_keys=True))\n",
        "                else:\n",
        "                    # Sort regular list (like list of strings)\n",
        "                    obj[key] = sorted(val)\n",
        "        return obj\n",
        "\n",
        "    gpt_output = sort_lists(gpt_output)\n",
        "    ground_truth = sort_lists(ground_truth)\n",
        "\n",
        "    # Pretty print JSON\n",
        "    print(f\"{title_1}:\\n```json\\n{json.dumps(gpt_output, indent=4)}\\n```\")\n",
        "    print(f\"\\n{title_2}:\\n```json\\n{json.dumps(ground_truth, indent=4)}\\n```\")\n",
        "\n",
        "\n",
        "# Extract the first valid JSON object from GPT response\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"\n",
        "    Extracts the first valid JSON object from a given text string.\n",
        "\n",
        "    Handles:\n",
        "    - Extra explanations around the JSON\n",
        "    - Markdown code blocks (e.g., ```json)\n",
        "    - Only returns the first complete JSON object\n",
        "\n",
        "    Returns:\n",
        "        str: JSON string\n",
        "    Raises:\n",
        "        ValueError: If no valid JSON is found\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Clean markdown code fences\n",
        "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
        "\n",
        "    # Match JSON object using balanced braces\n",
        "    brace_stack = []\n",
        "    start = None\n",
        "\n",
        "    for i, char in enumerate(text):\n",
        "        if char == '{':\n",
        "            if not brace_stack:\n",
        "                start = i\n",
        "            brace_stack.append('{')\n",
        "        elif char == '}':\n",
        "            if brace_stack:\n",
        "                brace_stack.pop()\n",
        "                if not brace_stack and start is not None:\n",
        "                    return text[start:i+1]\n",
        "\n",
        "    raise ValueError(\"No valid JSON object found in the text.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ObfA9c1HrGg"
      },
      "source": [
        "## 4. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Tr380EHrt3"
      },
      "outputs": [],
      "source": [
        "# Field-by-field value Accuracy\n",
        "def compare_parties(pred, truth):\n",
        "    \"\"\"\n",
        "    Compare two lists of 'party' entities using Jaccard similarity.\n",
        "\n",
        "    Works for both:\n",
        "    - List of strings ([\"Company_A\", \"Company_B\"])\n",
        "    - List of dictionaries ([{\"name\": ..., \"type\": ...}, ...])\n",
        "\n",
        "    Returns:\n",
        "        float: Jaccard similarity between predicted and ground truth sets.\n",
        "    \"\"\"\n",
        "    # Convert dicts to sorted JSON strings for comparison, leave strings as-is\n",
        "    def normalize_party_list(party_list):\n",
        "        normalized = []\n",
        "        for item in party_list:\n",
        "            if isinstance(item, dict):\n",
        "                normalized.append(json.dumps(item, sort_keys=True))  # Convert dict to consistent string\n",
        "            else:\n",
        "                normalized.append(str(item))\n",
        "        return set(normalized)\n",
        "\n",
        "    pred_set = normalize_party_list(pred)\n",
        "    truth_set = normalize_party_list(truth)\n",
        "\n",
        "    if not pred_set and not truth_set:\n",
        "        return 1.0\n",
        "    if not pred_set or not truth_set:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = pred_set & truth_set\n",
        "    union = pred_set | truth_set\n",
        "    return round(len(intersection) / len(union), 2)\n",
        "\n",
        "\n",
        "def evaluate_metadata_fields(gpt_output, ground_truth):\n",
        "    \"\"\"\n",
        "    Evaluate metadata extraction quality by comparing the GPT output with the ground truth on a per-field basis.\n",
        "\n",
        "    This function checks the accuracy of each metadata item:\n",
        "    - effective_date, jurisdiction, party, term\n",
        "    - Handles optional fields (no penalty if both are missing)\n",
        "    - For 'party', it allows partial credit using set overlap (Jaccard similarity)\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with per-field accuracy scores:\n",
        "        - 1.0 for exact matches\n",
        "        - 0.0 for mismatches\n",
        "        - partial value for list overlaps (e.g., 'party')\n",
        "        - None if both fields are missing\n",
        "    \"\"\"\n",
        "    if isinstance(gpt_output, str):\n",
        "        gpt_output = json.loads(gpt_output)\n",
        "    if isinstance(ground_truth, str):\n",
        "        ground_truth = json.loads(ground_truth)\n",
        "\n",
        "    fields = ['effective_date', 'jurisdiction', 'party', 'term']\n",
        "    field_scores = {}\n",
        "\n",
        "    for field in fields:\n",
        "        gpt_val = gpt_output.get(field)\n",
        "        gt_val = ground_truth.get(field)\n",
        "\n",
        "        if gpt_val is None and gt_val is None:\n",
        "            field_scores[field] = None  # No penalty if missing in both\n",
        "        elif gpt_val is None or gt_val is None:\n",
        "            field_scores[field] = 0.0\n",
        "        else:\n",
        "            if field != \"party\":\n",
        "                field_scores[field] = 1.0 if gpt_val == gt_val else 0.0\n",
        "            else:\n",
        "                field_scores[field] = compare_parties(gpt_val, gt_val)\n",
        "\n",
        "    return field_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C1rYQiKH1bB"
      },
      "outputs": [],
      "source": [
        "# Evaluate key_match and party_count\n",
        "def evaluate_key_match_and_party_count(gpt_output, true_keys_str, true_party_count):\n",
        "    \"\"\"\n",
        "    Evaluate the structural correctness of GPT output based on:\n",
        "    - Whether all expected metadata fields (keys) are present\n",
        "    - Whether the number of 'party' entities matches the ground truth\n",
        "\n",
        "    Parameters:\n",
        "        gpt_output: The extracted metadata from GPT\n",
        "        true_keys_str: A comma-separated string of expected keys (from df['keys'])\n",
        "        true_party_count: The number of expected parties (from df['party_count'])\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with:\n",
        "        - key_match_score: Fraction of required keys that GPT included\n",
        "        - party_count_score: 1 if correct number of parties, 0 otherwise\n",
        "        - supporting details like predicted/expected keys and party counts\n",
        "    \"\"\"\n",
        "    if isinstance(gpt_output, str):\n",
        "        gpt_output = json.loads(gpt_output)\n",
        "\n",
        "    expected_keys = set(true_keys_str.split(','))\n",
        "    gpt_keys = set(gpt_output.keys())\n",
        "\n",
        "    matched_keys = expected_keys & gpt_keys\n",
        "    key_match_score = round(len(matched_keys) / len(expected_keys), 2)\n",
        "\n",
        "    predicted_party_count = len(gpt_output.get('party', [])) if 'party' in gpt_output else 0\n",
        "    party_count_score = 1.0 if predicted_party_count == true_party_count else 0.0\n",
        "\n",
        "    return {\n",
        "        \"key_match_score\": key_match_score,\n",
        "        \"party_count_score\": party_count_score,\n",
        "        \"expected_keys\": expected_keys,\n",
        "        \"returned_keys\": gpt_keys,\n",
        "        \"expected_party_count\": true_party_count,\n",
        "        \"predicted_party_count\": predicted_party_count\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SuYwBGbH2EA"
      },
      "outputs": [],
      "source": [
        "def summarize_evaluation_across_samples(all_field_scores, all_structural_scores):\n",
        "    \"\"\"\n",
        "    Summarizes evaluation metrics across multiple samples.\n",
        "    Handles missing fields and avoids ZeroDivisionError.\n",
        "    \"\"\"\n",
        "    total_field_scores = {'effective_date': [], 'jurisdiction': [], 'party': [], 'term': []}\n",
        "    total_structural_scores = {'key_match': [], 'party_count': []}\n",
        "\n",
        "    print(\"\\n\\n================== üßæ Evaluation Summary Across Samples ==================\\n\")\n",
        "\n",
        "    for i, (field_scores, structural) in enumerate(zip(all_field_scores, all_structural_scores)):\n",
        "        print(f\"\\nüìÑ Sample {i+1}\")\n",
        "\n",
        "        # --- Field-Level Scores ---\n",
        "        field_sum = 0\n",
        "        field_count = 0\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"- {field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"- {field}: {score}\")\n",
        "                total_field_scores[field].append(score)\n",
        "                field_sum += score\n",
        "                field_count += 1\n",
        "\n",
        "        avg_field_score = round(field_sum / field_count, 3) if field_count > 0 else \"N/A\"\n",
        "\n",
        "        # --- Structural Scores ---\n",
        "        key_score = structural.get('key_match_score')\n",
        "        party_score = structural.get('party_count_score')\n",
        "\n",
        "        if key_score is not None:\n",
        "            total_structural_scores['key_match'].append(key_score)\n",
        "        if party_score is not None:\n",
        "            total_structural_scores['party_count'].append(party_score)\n",
        "\n",
        "        print(f\"- Key Match Score: {key_score}\")\n",
        "        print(f\"- Party Count Score: {party_score}\")\n",
        "\n",
        "        combined_structural = [\n",
        "            s for s in [key_score, party_score] if s is not None\n",
        "        ]\n",
        "        avg_structural = round(sum(combined_structural) / len(combined_structural), 3) if combined_structural else \"N/A\"\n",
        "\n",
        "        print(f\"üîπ Avg Field Score: {avg_field_score}\")\n",
        "        print(f\"üî∏ Combined Structural Score: {avg_structural}\")\n",
        "\n",
        "    # --- Aggregate Averages ---\n",
        "    print(\"\\n================== üìä AVERAGES ==================\\n\")\n",
        "\n",
        "    print(\"üîπ Field-Level Averages:\")\n",
        "    for field, scores in total_field_scores.items():\n",
        "        if scores:\n",
        "            avg = round(sum(scores) / len(scores), 3)\n",
        "            print(f\"- {field}: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {field}: (No available comparisons)\")\n",
        "\n",
        "    print(\"\\nüî∏ Structural Averages:\")\n",
        "    for k in total_structural_scores:\n",
        "        scores = total_structural_scores[k]\n",
        "        if scores:\n",
        "            avg = round(sum(scores) / len(scores), 3)\n",
        "            label = \"Key Match\" if k == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: {avg}\")\n",
        "        else:\n",
        "            label = \"Key Match\" if k == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: (No available comparisons)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7UHQfmiotWY"
      },
      "outputs": [],
      "source": [
        "def print_average_scores_only(all_field_scores, all_structural_scores):\n",
        "    \"\"\"\n",
        "    Print only the average field-level and structural scores across all samples.\n",
        "    Use this when you want a compact summary view.\n",
        "    \"\"\"\n",
        "    print(\"\\n================== üìä AVERAGE METRICS ==================\\n\")\n",
        "\n",
        "    # --- Field-Level Averages ---\n",
        "    field_totals = {}\n",
        "    field_counts = {}\n",
        "\n",
        "    for sample in all_field_scores:\n",
        "        for field, score in sample.items():\n",
        "            if score is not None:\n",
        "                field_totals[field] = field_totals.get(field, 0) + score\n",
        "                field_counts[field] = field_counts.get(field, 0) + 1\n",
        "\n",
        "    print(\"üîπ Field-Level Averages:\")\n",
        "    for field in ['effective_date', 'jurisdiction', 'party', 'term']:\n",
        "        if field_counts.get(field, 0) > 0:\n",
        "            avg = round(field_totals[field] / field_counts[field], 3)\n",
        "            print(f\"- {field}: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {field}: (No available comparisons)\")\n",
        "\n",
        "    # --- Structural Averages ---\n",
        "    total_structural_scores = {\n",
        "        \"key_match\": [],\n",
        "        \"party_count\": []\n",
        "    }\n",
        "\n",
        "    for result in all_structural_scores:\n",
        "        total_structural_scores[\"key_match\"].append(result[\"key_match_score\"])\n",
        "        total_structural_scores[\"party_count\"].append(result[\"party_count_score\"])\n",
        "\n",
        "    print(\"\\nüî∏ Structural Averages:\")\n",
        "    for metric_key, values in total_structural_scores.items():\n",
        "        if values:\n",
        "            avg = round(sum(values) / len(values), 3)\n",
        "            label = \"Key Match\" if metric_key == \"key_match\" else \"Party Count\"\n",
        "            print(f\"- {label} Score: {avg}\")\n",
        "        else:\n",
        "            print(f\"- {metric_key} Score: (No data)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15jwi0mH6HG"
      },
      "source": [
        "## 5. Structured Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLY5ldHdH8vS"
      },
      "outputs": [],
      "source": [
        "# Define persona, instructions, and formatting rules\n",
        "persona = \"\"\"\n",
        "You are an expert in identifying and extracting metadata from NDA (Non-Disclosure Agreement) documents.\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"\"\"\n",
        "Extract the following metadata:\n",
        "- effective_date: When the agreement becomes legally binding\n",
        "- jurisdiction: The governing legal territory\n",
        "- party: Entities bound by the agreement (May have multiple values)\n",
        "- term: Duration of the agreement\n",
        "Return only the metadata fields that are explicitly present in the text. If a field like effective_date or term is missing, exclude it from the output JSON and do not guess or explain or return null.\n",
        "\"\"\"\n",
        "\n",
        "data_format = \"\"\"\n",
        "Output must be a JSON object following these Rules:\n",
        "- Replace all spaces and colons (:) in attribute values with underscores (_)\n",
        "- Dates must be in 'YYYY-MM-DD' format\n",
        "- Jurisdiction: Only the state or country name is included, without prefixes like \"State of\"\n",
        "- Party names must follow Title_Case with underscores replacing spaces and corporate suffixes standardized (e.g., \"Inc.\", \"Corp.\", \"LLC\", \"Ltd.\")\n",
        "- Duration (term) must be normalized to 'number_units' format (e.g., 2_years, 12_months)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33zPfUJOIBBE"
      },
      "source": [
        "## 6. Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8UaSEWVRwE9",
        "outputId": "b8aef4b0-00d3-417d-8116-4fdf798d1453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.60s/it, est. speed input: 1063.59 toks/s, output: 70.65 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"Heidrick & Struggles, Inc\",\n",
            "            \"type\": \"Delaware_Corporation\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"Second_Party\",\n",
            "            \"type\": \"\"\n",
            "        }\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.83s/it, est. speed input: 1124.12 toks/s, output: 66.87 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"Requesting Stockholder\",\n",
            "            \"name\": \"Elaine P. Wynn\",\n",
            "            \"type\": \"individual\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"\",\n",
            "            \"name\": \"Wynn Resorts, Limited\",\n",
            "            \"type\": \"corporation\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.19s/it, est. speed input: 1031.09 toks/s, output: 67.12 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"GTCR LLC\",\n",
            "            \"name\": \"GTCR LLC\",\n",
            "            \"type\": \"LLC\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"Vocus, Inc.\",\n",
            "            \"name\": \"Vocus, Inc.\",\n",
            "            \"type\": \"Inc.\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"Gtcr_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 0.75\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.11s/it, est. speed input: 1238.42 toks/s, output: 65.88 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2005-11-01\",\n",
            "    \"jurisdiction\": \"New Jersey\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"RENAISSANCE BRANDS LTD.\",\n",
            "            \"name\": \"RENAISSANCE BRANDS LTD.\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"VITAMIN SHOPPE INDUSTRIES INC.\",\n",
            "            \"name\": \"VITAMIN SHOPPE INDUSTRIES INC.\"\n",
            "        }\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 0.0\n",
            "jurisdiction: 0.0\n",
            "party: 0.0\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.75\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.667\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 0.75\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.667\n",
            "üî∏ Combined Structural Score: 0.875\n",
            "\n",
            "üìÑ Sample 4\n",
            "- effective_date: 0.0\n",
            "- jurisdiction: 0.0\n",
            "- party: 0.0\n",
            "- term: 0.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.0\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.75\n",
            "- jurisdiction: 0.75\n",
            "- party: 0.0\n",
            "- term: 0.5\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.938\n",
            "- Party Count Score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Zero-shot evaluation: no examples included in the prompt\n",
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "# Set up Mistral sampling\n",
        "from vllm import SamplingParams\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    # Zero-shot prompt\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract clean JSON from model response\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        # Display Model Output vs Ground Truth\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        # Evaluation Method 1: Field-Level Accuracy\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        # Evaluation Method 2: Structural Correctness\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        # Fix: convert to dict if still string\n",
        "        if isinstance(extracted_json, str):\n",
        "            extracted_json = json.loads(extracted_json)\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è Model output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary for zero-shot\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9qz_mWycqyL"
      },
      "source": [
        "## 7. Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxf4Cn5fcnMK",
        "outputId": "48ecee79-5fa8-483e-a8ab-a699567874b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 7356.43 toks/s, output: 47.10 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_&_Struggles_Inc.\",\n",
            "        \"Second_Party\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 10126.66 toks/s, output: 56.25 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Limited\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.33\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 13212.12 toks/s, output: 51.90 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è GPT Output is not valid JSON\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "‚ö†Ô∏è Model output is not valid JSON\n",
            "Raw output:\n",
            " {\n",
            "\"jurisdiction\": \"Delaware\"\n",
            "}\n",
            "\n",
            "The effective_date is not explicitly stated in the text, so it should not be included in the output.\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 7661.62 toks/s, output: 56.62 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ GPT Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2005-11-01\",\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"RENAISSANCE_BRANDS_LTD.\",\n",
            "        \"VITAMIN_SHOPPE_INDUSTRIES_INC.\"\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 0.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.75\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.33\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.777\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 0.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 0.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.25\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.667\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.11\n",
            "- term: 0.5\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "examples = \"\"\n",
        "for i in range(2):  # Adjust number of few-shot examples here\n",
        "    text_sample = df['text'][i]\n",
        "    metadata_sample = df['extracted'][i]\n",
        "    examples += f\"\\n\\nExample {i+1}\\ntext = {text_sample}\\noutput = {metadata_sample}\"\n",
        "\n",
        "# Main prompt test on selected samples\n",
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "# Set up Mistral sampling\n",
        "from vllm import SamplingParams\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    # Zero-shot prompt\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Here are some examples of NDA metadata extraction:{examples}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract clean JSON from model response\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        # Display Model Output vs Ground Truth\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        # Evaluation Method 1: Field-Level Accuracy\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        # Evaluation Method 2: Structural Correctness\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        # Fix: convert to dict if still string\n",
        "        if isinstance(extracted_json, str):\n",
        "            extracted_json = json.loads(extracted_json)\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è Model output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary for zero-shot\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXLXM2iMlNhK"
      },
      "source": [
        "## 8. Chain-of-Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZOIjI1klQBh",
        "outputId": "a2aae98b-75b1-4fee-e9fa-6440afd22f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================= Text 8 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it, est. speed input: 1094.74 toks/s, output: 70.55 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"Heidrick & Struggles, Inc.\",\n",
            "            \"name\": \"Heidrick_Struggles_Inc\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"Second Party\",\n",
            "            \"name\": \"Second_Party\"\n",
            "        }\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2011-05-16\",\n",
            "    \"jurisdiction\": \"Illinois\",\n",
            "    \"party\": [\n",
            "        \"Heidrick_and_Struggles_Inc.\",\n",
            "        \"Richard_W._Pehlke\"\n",
            "    ],\n",
            "    \"term\": \"5_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 1.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 20 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 1004.98 toks/s, output: 70.88 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"Requesting Stockholder\",\n",
            "            \"name\": \"Elaine P. Wynn\",\n",
            "            \"type\": \"individual\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"\",\n",
            "            \"name\": \"Wynn Resorts, Limited\",\n",
            "            \"type\": \"corporation\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2018-04-20\",\n",
            "    \"jurisdiction\": \"Nevada\",\n",
            "    \"party\": [\n",
            "        \"Elaine_P._Wynn\",\n",
            "        \"Wynn_Resorts_Ltd.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: (Not present in either) ‚úÖ\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 55 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.75s/it, est. speed input: 1382.26 toks/s, output: 70.86 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"GTCR LLC\",\n",
            "            \"name\": \"GTCR LLC\",\n",
            "            \"type\": \"LLC\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"Vocus, Inc.\",\n",
            "            \"name\": \"Vocus, Inc.\",\n",
            "            \"type\": \"Company\"\n",
            "        }\n",
            "    ],\n",
            "    \"term\": \"Not specified\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2014-04-06\",\n",
            "    \"jurisdiction\": \"Delaware\",\n",
            "    \"party\": [\n",
            "        \"Gtcr_LLC\",\n",
            "        \"Vocus_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 1.0\n",
            "jurisdiction: 1.0\n",
            "party: 0.0\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "======================= Text 70 =======================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.99s/it, est. speed input: 1396.83 toks/s, output: 69.99 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Mistralai Output:\n",
            "```json\n",
            "{\n",
            "    \"effective_date\": \"2005-11-01\",\n",
            "    \"jurisdiction\": \"New Jersey\",\n",
            "    \"party\": [\n",
            "        {\n",
            "            \"title\": \"RENAISSANCE BRANDS LTD.\",\n",
            "            \"name\": \"RENAISSANCE BRANDS LTD.\"\n",
            "        },\n",
            "        {\n",
            "            \"title\": \"VITAMIN SHOPPE INDUSTRIES INC.\",\n",
            "            \"name\": \"VITAMIN SHOPPE INDUSTRIES INC.\"\n",
            "        }\n",
            "    ],\n",
            "    \"term\": \"2_years\"\n",
            "}\n",
            "```\n",
            "\n",
            "üî∏ Ground Truth:\n",
            "```json\n",
            "{\n",
            "    \"jurisdiction\": \"New_Jersey\",\n",
            "    \"party\": [\n",
            "        \"Renaissance_Brands_Ltd.\",\n",
            "        \"Vitamin_Shoppe_Industuries_Inc.\"\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "üìä Field-Level Evaluation:\n",
            "effective_date: 0.0\n",
            "jurisdiction: 0.0\n",
            "party: 0.0\n",
            "term: 0.0\n",
            "\n",
            "üîç Structural Evaluation:\n",
            "Key Match Score: 1.0\n",
            "Party Count Score: 1.0\n",
            "Expected Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Returned Keys: {'effective_date', 'term', 'jurisdiction', 'party'}\n",
            "Expected Party Count: 2\n",
            "Predicted Party Count: 2\n",
            "\n",
            "\n",
            "================== üßæ Evaluation Summary Across Samples ==================\n",
            "\n",
            "\n",
            "üìÑ Sample 1\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 1.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.75\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 2\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: (Not present in either) ‚úÖ\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.667\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 3\n",
            "- effective_date: 1.0\n",
            "- jurisdiction: 1.0\n",
            "- party: 0.0\n",
            "- term: 0.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.5\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "üìÑ Sample 4\n",
            "- effective_date: 0.0\n",
            "- jurisdiction: 0.0\n",
            "- party: 0.0\n",
            "- term: 0.0\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n",
            "üîπ Avg Field Score: 0.0\n",
            "üî∏ Combined Structural Score: 1.0\n",
            "\n",
            "================== üìä AVERAGES ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.75\n",
            "- jurisdiction: 0.75\n",
            "- party: 0.0\n",
            "- term: 0.333\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 1.0\n",
            "- Party Count Score: 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "example_indices = [ 8, 20, 55, 70 ]\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "\n",
        "# Set up Mistral sampling\n",
        "from vllm import SamplingParams\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    # Zero-shot prompt\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Let‚Äôs extract the required metadata by thinking step-by-step.\n",
        "\n",
        "1. First, read the NDA and look for the effective_date. This is typically found where the agreement says something like \"effective as of\", \"made on\", or \"entered into on\".\n",
        "2. Next, identify the jurisdiction. This is usually mentioned in a clause like \"governed by the laws of...\" or \"jurisdiction of\".\n",
        "3. Then, extract all the parties. These are the legal entities entering into the agreement. They often appear at the beginning or near the signature lines.\n",
        "4. Finally, determine the term. Look for duration-related phrases like \"for two years\", \"valid until\", or \"will terminate after...\".\n",
        "\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\n\\n======================= Text {x} =======================\\n\")\n",
        "    model_output_raw = generate_response(prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract clean JSON from model response\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "\n",
        "        # Display Model Output vs Ground Truth\n",
        "        format_and_print_json(extracted_json, df['extracted'][x])\n",
        "\n",
        "        # Evaluation Method 1: Field-Level Accuracy\n",
        "        print(\"\\nüìä Field-Level Evaluation:\")\n",
        "        field_scores = evaluate_metadata_fields(extracted_json, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        for field, score in field_scores.items():\n",
        "            if score is None:\n",
        "                print(f\"{field}: (Not present in either) ‚úÖ\")\n",
        "            else:\n",
        "                print(f\"{field}: {score}\")\n",
        "\n",
        "        # Evaluation Method 2: Structural Correctness\n",
        "        print(\"\\nüîç Structural Evaluation:\")\n",
        "        # Fix: convert to dict if still string\n",
        "        if isinstance(extracted_json, str):\n",
        "            extracted_json = json.loads(extracted_json)\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            extracted_json,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        print(f\"Key Match Score: {key_eval['key_match_score']}\")\n",
        "        print(f\"Party Count Score: {key_eval['party_count_score']}\")\n",
        "        print(f\"Expected Keys: {key_eval['expected_keys']}\")\n",
        "        print(f\"Returned Keys: {key_eval['returned_keys']}\")\n",
        "        print(f\"Expected Party Count: {key_eval['expected_party_count']}\")\n",
        "        print(f\"Predicted Party Count: {key_eval['predicted_party_count']}\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"‚ö†Ô∏è Model output is not valid JSON\\nRaw output:\\n\", model_output_raw)\n",
        "\n",
        "# Final Summary for zero-shot\n",
        "summarize_evaluation_across_samples(all_field_scores, all_structural_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a-0ERa6npBB"
      },
      "source": [
        "## 9. Evaluation Using Entire Dataset (Few-shot Prompting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7Hs4E7qnp5T",
        "outputId": "8fd0af0b-3c5f-4c97-d008-74f040a70bf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.37s/it, est. speed input: 5871.96 toks/s, output: 55.43 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 5191.05 toks/s, output: 60.97 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 7989.75 toks/s, output: 53.49 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 6928.19 toks/s, output: 59.03 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.22s/it, est. speed input: 6804.56 toks/s, output: 53.37 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 7684.90 toks/s, output: 52.30 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.11it/s, est. speed input: 6993.10 toks/s, output: 62.09 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 7693.12 toks/s, output: 56.10 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 6672.51 toks/s, output: 60.98 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 7141.45 toks/s, output: 52.90 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.51s/it, est. speed input: 4808.94 toks/s, output: 60.88 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 7515.75 toks/s, output: 53.18 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 6877.08 toks/s, output: 52.12 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it, est. speed input: 4688.00 toks/s, output: 58.62 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.43s/it, est. speed input: 4388.26 toks/s, output: 65.22 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 7050.34 toks/s, output: 56.93 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.54s/it, est. speed input: 5509.68 toks/s, output: 56.43 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 8251.23 toks/s, output: 51.99 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.16s/it, est. speed input: 7570.72 toks/s, output: 50.03 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 7482.49 toks/s, output: 56.24 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 7016.47 toks/s, output: 58.85 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 7277.68 toks/s, output: 51.38 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it, est. speed input: 7207.34 toks/s, output: 53.30 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.15s/it, est. speed input: 3984.48 toks/s, output: 59.58 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.24s/it, est. speed input: 7166.91 toks/s, output: 51.44 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 5752.67 toks/s, output: 61.79 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.75s/it, est. speed input: 1005.56 toks/s, output: 66.10 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.26s/it, est. speed input: 6159.59 toks/s, output: 57.18 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 5843.38 toks/s, output: 65.00 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 4522.66 toks/s, output: 63.37 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 6782.82 toks/s, output: 57.09 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.13s/it, est. speed input: 6442.22 toks/s, output: 58.58 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 6370.25 toks/s, output: 57.72 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 7449.97 toks/s, output: 55.94 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 6994.70 toks/s, output: 53.79 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 6657.93 toks/s, output: 52.70 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 6924.30 toks/s, output: 57.07 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 8553.57 toks/s, output: 48.57 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 7781.62 toks/s, output: 57.42 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 6516.93 toks/s, output: 61.40 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.33s/it, est. speed input: 6566.28 toks/s, output: 53.25 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.26s/it, est. speed input: 5994.75 toks/s, output: 58.71 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 8281.96 toks/s, output: 50.84 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 7738.65 toks/s, output: 55.41 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 6410.02 toks/s, output: 55.87 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 7627.42 toks/s, output: 54.20 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 7056.46 toks/s, output: 51.84 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 7316.70 toks/s, output: 56.98 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 7634.69 toks/s, output: 54.39 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.28s/it, est. speed input: 6313.80 toks/s, output: 55.52 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.53s/it, est. speed input: 5059.02 toks/s, output: 59.01 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 6653.37 toks/s, output: 59.74 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.40s/it, est. speed input: 4755.13 toks/s, output: 62.82 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 7505.53 toks/s, output: 52.13 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 7575.38 toks/s, output: 57.44 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 6377.15 toks/s, output: 62.04 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 6807.35 toks/s, output: 58.50 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.00it/s, est. speed input: 8194.12 toks/s, output: 52.13 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 6013.80 toks/s, output: 58.48 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 5703.89 toks/s, output: 61.36 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 7488.12 toks/s, output: 52.97 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.32s/it, est. speed input: 6311.95 toks/s, output: 55.13 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 6706.63 toks/s, output: 58.92 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.13s/it, est. speed input: 6483.08 toks/s, output: 57.80 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.12s/it, est. speed input: 6598.46 toks/s, output: 57.42 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.56s/it, est. speed input: 5152.71 toks/s, output: 57.86 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.60s/it, est. speed input: 5301.50 toks/s, output: 56.89 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it, est. speed input: 6532.58 toks/s, output: 57.25 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.07s/it, est. speed input: 3867.05 toks/s, output: 60.80 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 6219.07 toks/s, output: 58.22 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 6736.12 toks/s, output: 57.10 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 4982.12 toks/s, output: 59.30 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.26s/it, est. speed input: 6679.51 toks/s, output: 54.17 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 8016.99 toks/s, output: 51.90 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.79s/it, est. speed input: 4925.48 toks/s, output: 56.48 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.76s/it, est. speed input: 5113.59 toks/s, output: 55.63 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 6908.80 toks/s, output: 58.14 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 6627.14 toks/s, output: 55.73 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 5758.45 toks/s, output: 58.60 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.10s/it, est. speed input: 7790.98 toks/s, output: 51.64 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.15s/it, est. speed input: 6832.83 toks/s, output: 55.45 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 6751.28 toks/s, output: 60.77 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.18s/it, est. speed input: 6798.14 toks/s, output: 55.22 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 6641.04 toks/s, output: 60.25 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 6687.28 toks/s, output: 59.19 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.61s/it, est. speed input: 4737.93 toks/s, output: 60.75 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 7350.83 toks/s, output: 54.14 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 7137.61 toks/s, output: 53.09 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.42s/it, est. speed input: 6188.07 toks/s, output: 53.73 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.20s/it, est. speed input: 3858.79 toks/s, output: 59.90 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.21s/it, est. speed input: 5960.41 toks/s, output: 59.32 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 7339.59 toks/s, output: 52.13 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.70s/it, est. speed input: 5182.11 toks/s, output: 56.01 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 5744.98 toks/s, output: 58.74 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 5775.62 toks/s, output: 60.69 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.68s/it, est. speed input: 4700.26 toks/s, output: 59.59 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 8450.85 toks/s, output: 50.97 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.01s/it, est. speed input: 3421.74 toks/s, output: 64.26 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.34s/it, est. speed input: 6396.03 toks/s, output: 54.72 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 4644.74 toks/s, output: 61.77 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 6732.11 toks/s, output: 58.65 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.08s/it, est. speed input: 1123.19 toks/s, output: 67.11 toks/s]\n",
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.38s/it, est. speed input: 5647.83 toks/s, output: 58.10 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================== üìä AVERAGE METRICS ==================\n",
            "\n",
            "üîπ Field-Level Averages:\n",
            "- effective_date: 0.693\n",
            "- jurisdiction: 0.989\n",
            "- party: 0.385\n",
            "- term: 0.256\n",
            "\n",
            "üî∏ Structural Averages:\n",
            "- Key Match Score: 0.98\n",
            "- Party Count Score: 0.795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create few-shot examples using first 2 rows\n",
        "examples = \"\"\n",
        "for i in range(2):\n",
        "    text_sample = df['text'][i]\n",
        "    metadata_sample = df['extracted'][i]\n",
        "    examples += f\"\\n\\nExample {i+1}\\ntext = {text_sample}\\noutput = {metadata_sample}\"\n",
        "\n",
        "# Prepare indices and tracking\n",
        "example_indices = df.index.tolist()\n",
        "all_field_scores = []\n",
        "all_structural_scores = []\n",
        "results = []\n",
        "\n",
        "# Set up Mistral sampling\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "# Loop through all examples\n",
        "for x in example_indices:\n",
        "    input_text = df['text'][x]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "{persona}\n",
        "\n",
        "{instruction}\n",
        "\n",
        "{data_format}\n",
        "\n",
        "Here are some examples of NDA metadata extraction:{examples}\n",
        "\n",
        "Now extract metadata from the following NDA:\n",
        "text = {input_text}\n",
        "output =\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        model_output_raw = generate_response(prompt)\n",
        "        extracted_json = extract_json_from_text(model_output_raw)\n",
        "        normalized_output = normalize_output(extracted_json)\n",
        "\n",
        "        if isinstance(normalized_output, str):\n",
        "            normalized_output = json.loads(normalized_output)\n",
        "\n",
        "        field_scores = evaluate_metadata_fields(normalized_output, df['extracted'][x])\n",
        "        all_field_scores.append(field_scores)\n",
        "\n",
        "        key_eval = evaluate_key_match_and_party_count(\n",
        "            normalized_output,\n",
        "            df['keys'][x],\n",
        "            df['party_count'][x]\n",
        "        )\n",
        "        all_structural_scores.append(key_eval)\n",
        "\n",
        "        ground_truth = df['extracted'][x]\n",
        "        if isinstance(ground_truth, str):\n",
        "            ground_truth = json.loads(ground_truth)\n",
        "\n",
        "        model_keys = list(normalized_output.keys())\n",
        "        model_party_count = len(normalized_output.get(\"party\", [])) if \"party\" in normalized_output else 0\n",
        "\n",
        "        results.append({\n",
        "            \"index\": x,\n",
        "            \"model_output\": json.dumps(normalized_output),\n",
        "            \"ground_truth\": json.dumps(ground_truth),\n",
        "            \"model_keys\": \",\".join(model_keys),\n",
        "            \"true_keys\": df['keys'][x],\n",
        "            \"model_party_count\": model_party_count,\n",
        "            \"true_party_count\": df['party_count'][x]\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            \"index\": x,\n",
        "            \"model_output\": f\"ERROR: {str(e)}\",\n",
        "            \"ground_truth\": df['extracted'][x],\n",
        "            \"model_keys\": \"\",\n",
        "            \"true_keys\": df['keys'][x],\n",
        "            \"model_party_count\": \"\",\n",
        "            \"true_party_count\": df['party_count'][x]\n",
        "        })\n",
        "\n",
        "# Create DataFrame and export results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"mistral.csv\", index=False)\n",
        "\n",
        "# Show evaluation summary\n",
        "print_average_scores_only(all_field_scores, all_structural_scores)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32064c5476a447ba9038ea4d4190f339": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "351b91f723e44be3bc83c6bacd8bc220": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36706232689d43b087b2e8f6123cf189": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351b91f723e44be3bc83c6bacd8bc220",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5b85866acd441efb7b3415a1318c165",
            "value": 3
          }
        },
        "60e9d2caf01e4695bb857487bfb6f099": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d1dc03053d84f6a9534c91a4f761878": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ece49005f4643c684953f1e90b40d40",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_60e9d2caf01e4695bb857487bfb6f099",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá3/3‚Äá[00:07&lt;00:00,‚Äá‚Äá2.51s/it]\n"
          }
        },
        "8ece49005f4643c684953f1e90b40d40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c89772d0ec48bc96576e40fe15668f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b85866acd441efb7b3415a1318c165": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d19635d104964246a6a12171ebd5c908": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f00c23a9cbe047c68bc0c434c8c0594e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_32064c5476a447ba9038ea4d4190f339",
            "value": ""
          }
        },
        "d94feecce5e94d19856fc8a768c9d39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d19635d104964246a6a12171ebd5c908",
              "IPY_MODEL_36706232689d43b087b2e8f6123cf189",
              "IPY_MODEL_6d1dc03053d84f6a9534c91a4f761878"
            ],
            "layout": "IPY_MODEL_b8c89772d0ec48bc96576e40fe15668f"
          }
        },
        "f00c23a9cbe047c68bc0c434c8c0594e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
